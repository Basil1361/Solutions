<!-- Linear Regression - Complete Beginner's Guide -->

# 1. Introduction: What is Linear Regression?

## The Basic Idea
Imagine you want to predict the **price of a house** based on its **number of bedrooms**.

### First Attempt (Too Simple):
```
Price = Number of Bedrooms
```
**Problem:** If a house has 3 bedrooms, the price would be $3? That makes no sense!

### Better Attempt (Still Simple):
```
Price = Œ≤ √ó Number of Bedrooms
```
Where **Œ≤ (beta)** is a "weight" or "multiplier"
- Example: If Œ≤ = $100,000, then 3 bedrooms = $300,000
- **This is called Simple Linear Regression**

**Why it's not perfect:** In reality, data points don't fall on a perfect line - they're scattered around it (scatterplot).

---

# 2. Real World: Multiple Features

## Houses depend on many factors:
- **x‚ÇÅ** = number of bedrooms
- **x‚ÇÇ** = location quality (1-10 scale)
- **x‚ÇÉ** = house age
- **x‚ÇÑ** = square footage
- etc.

## The Formula:
```
y = Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + Œ≤‚ÇÉx‚ÇÉ + ... + Œµ
```

**Where:**
- **y** = actual price (what we observe)
- **Œ≤‚ÇÅ, Œ≤‚ÇÇ, Œ≤‚ÇÉ** = weights (how much each feature matters)
- **x‚ÇÅ, x‚ÇÇ, x‚ÇÉ** = features (bedrooms, location, age, etc.)
- **Œµ (epsilon)** = error term (random noise we can't predict)

## Matrix Notation (Compact Form):
Instead of writing out every term, we use matrices:

**Model (with error):**
```
y = XŒ≤ + Œµ
```

**Prediction (our guess, without error):**
```
≈∑ = XŒ≤
```

**Example:**
```
X = [1  3  8]    Œ≤ = [Œ≤‚ÇÄ]    y = [300000]
    [1  4  7]        [Œ≤‚ÇÅ]        [400000]
    [1  2  9]        [Œ≤‚ÇÇ]        [250000]
    
Column meanings:
- Column 0: bias term (always 1)
- Column 1: number of bedrooms
- Column 2: location quality
```

---

# 3. Main Objective: Minimize Error

## What is a Loss Function?
A **loss function** measures how wrong our predictions are. We want to minimize it!

### Mean Squared Error (MSE) - Most Common:
```
MSE = (1/n) Œ£·µ¢‚Çå‚ÇÅ‚Åø (y·µ¢ - ≈∑·µ¢)¬≤
```

**Breaking it down:**
- **y·µ¢** = actual price of house i
- **≈∑·µ¢** = our predicted price of house i
- **(y·µ¢ - ≈∑·µ¢)** = error (how much we're off)
- **(y·µ¢ - ≈∑·µ¢)¬≤** = squared error (makes all errors positive, punishes big errors more)
- **Œ£** = sum over all n houses
- **(1/n)** = average the errors

**Why square the error?**
1. Makes negative errors positive (|-5| and |+5| both become 25)
2. Punishes large errors more (error of 10 is 100, but error of 2 is only 4)
3. Makes the math easier (derivatives are cleaner)

### Mean Absolute Error (MAE) - Alternative:
```
MAE = (1/n) Œ£·µ¢‚Çå‚ÇÅ‚Åø |y·µ¢ - ≈∑·µ¢|
```
- Just takes absolute value instead of squaring
- Less common because math is harder

### Matrix Form of MSE:
Instead of writing the sum, we can use matrix notation:

```
Loss = (1/2n) Œ£·µ¢‚Çå‚ÇÅ‚Åø (≈∑·µ¢ - y·µ¢)¬≤
Loss = (1/2n) (XŒ≤ - y)·µÄ(XŒ≤ - y)
```

**Why (1/2n) instead of (1/n)?**
- The factor of 1/2 makes the derivative cleaner (the 2 will cancel out later)
- It doesn't change where the minimum is, just scales the function

**What does (XŒ≤ - y)·µÄ(XŒ≤ - y) mean?**
- **(XŒ≤ - y)** = vector of all errors [error‚ÇÅ, error‚ÇÇ, ..., error‚Çô]
- **·µÄ** means transpose (flip rows to columns)
- **(XŒ≤ - y)·µÄ(XŒ≤ - y)** = dot product = sum of squared errors

---

# 4. Deriving the Best Œ≤ (The Math Journey!)

## Goal: Find Œ≤ that minimizes Loss

### Step 1: Expand the Matrix Expression
```
Loss = (1/2n) (XŒ≤ - y)·µÄ(XŒ≤ - y)
```

**Let's expand (XŒ≤ - y)·µÄ(XŒ≤ - y):**
```
(XŒ≤ - y)·µÄ(XŒ≤ - y) = (XŒ≤)·µÄ(XŒ≤) - (XŒ≤)·µÄy - y·µÄ(XŒ≤) + y·µÄy
```

**Why? Using FOIL (First, Outer, Inner, Last):**
- **(a - b)(a - b) = a¬≤ - ab - ba + b¬≤**
- In matrices: **(XŒ≤ - y)·µÄ(XŒ≤ - y) = (XŒ≤)·µÄ(XŒ≤) - (XŒ≤)·µÄy - y·µÄ(XŒ≤) + y·µÄy**

**Simplify (XŒ≤)·µÄy and y·µÄ(XŒ≤):**
- Both are scalars (single numbers), not matrices
- For scalars: a·µÄb = b·µÄa
- So: (XŒ≤)·µÄy = y·µÄ(XŒ≤)
- Therefore: -(XŒ≤)·µÄy - y·µÄ(XŒ≤) = -2(XŒ≤)·µÄy = -2y·µÄ(XŒ≤)

**Using matrix property (AB)·µÄ = B·µÄA·µÄ:**
```
(XŒ≤)·µÄ(XŒ≤) = Œ≤·µÄX·µÄXŒ≤
(XŒ≤)·µÄy = Œ≤·µÄX·µÄy
```

**Final simplified form:**
```
Loss = (1/2n) (Œ≤·µÄX·µÄXŒ≤ - 2Œ≤·µÄX·µÄy + y·µÄy)
```

---

### Step 2: Take the Derivative with Respect to Œ≤

**Goal:** Find where Loss is minimum ‚Üí Take derivative and set to 0

**Matrix Calculus Rules we need:**
1. **‚àÇ(a·µÄŒ≤)/‚àÇŒ≤ = a** (derivative of linear term)
2. **‚àÇ(Œ≤·µÄAŒ≤)/‚àÇŒ≤ = 2AŒ≤** (derivative of quadratic term, if A is symmetric)
3. **‚àÇ(constant)/‚àÇŒ≤ = 0** (derivative of constant)

**Apply to our Loss function:**
```
‚àÇLoss/‚àÇŒ≤ = (1/2n) [‚àÇ(Œ≤·µÄX·µÄXŒ≤)/‚àÇŒ≤ - ‚àÇ(2Œ≤·µÄX·µÄy)/‚àÇŒ≤ + ‚àÇ(y·µÄy)/‚àÇŒ≤]
```

**Term by term:**
1. **‚àÇ(Œ≤·µÄX·µÄXŒ≤)/‚àÇŒ≤ = 2X·µÄXŒ≤** (using rule 2, since X·µÄX is symmetric)
2. **‚àÇ(2Œ≤·µÄX·µÄy)/‚àÇŒ≤ = 2X·µÄy** (using rule 1)
3. **‚àÇ(y·µÄy)/‚àÇŒ≤ = 0** (y·µÄy doesn't depend on Œ≤)

**Combine:**
```
‚àÇLoss/‚àÇŒ≤ = (1/2n) (2X·µÄXŒ≤ - 2X·µÄy)
‚àÇLoss/‚àÇŒ≤ = (1/n) (X·µÄXŒ≤ - X·µÄy)
```

**Notice:** The 2 in the denominator (1/2n) cancelled with the 2 from the derivative!

---

### Step 3: Set Derivative to Zero (Find Minimum)

At the minimum, the derivative = 0:
```
(1/n)(X·µÄXŒ≤ - X·µÄy) = 0
```

**Multiply both sides by n:**
```
X·µÄXŒ≤ - X·µÄy = 0
```

**Add X·µÄy to both sides:**
```
X·µÄXŒ≤ = X·µÄy
```

**This is called the "Normal Equation"**

---

### Step 4: Solve for Œ≤ (The Final Formula!)

**Multiply both sides by (X·µÄX)‚Åª¬π:**
```
(X·µÄX)‚Åª¬πX·µÄXŒ≤ = (X·µÄX)‚Åª¬πX·µÄy
```

**Since (X·µÄX)‚Åª¬πX·µÄX = I (identity matrix):**
```
IŒ≤ = (X·µÄX)‚Åª¬πX·µÄy
Œ≤ = (X·µÄX)‚Åª¬πX·µÄy
```

## üéâ THIS IS THE CLOSED-FORM SOLUTION! üéâ

---

# 5. What Does This Mean?

## The Formula: Œ≤ = (X·µÄX)‚Åª¬πX·µÄy

**In plain English:**
- Given training data **X** (features) and **y** (prices)
- We can directly calculate the **best Œ≤** that minimizes MSE
- No iteration needed - just matrix operations!

**Components:**
- **X·µÄ** = transpose of X (flip rows and columns)
- **X·µÄX** = a square matrix (dimensions: features √ó features)
- **(X·µÄX)‚Åª¬π** = inverse of X·µÄX (like division for matrices)
- **X·µÄy** = matrix-vector product

---

# 6. Python Example

```python
import numpy as np

# Sample data: [bias, bedrooms, location]
X = np.array([
    [1, 2, 8],  # House 1: 2 bedrooms, location 8/10
    [1, 3, 7],  # House 2: 3 bedrooms, location 7/10
    [1, 4, 9],  # House 3: 4 bedrooms, location 9/10
    [1, 2, 6]   # House 4: 2 bedrooms, location 6/10
])

y = np.array([200000, 300000, 400000, 180000])  # Actual prices

# Calculate Œ≤ using the formula: Œ≤ = (X·µÄX)‚Åª¬πX·µÄy
beta = np.linalg.inv(X.T @ X) @ X.T @ y

print(f"Œ≤ = {beta}")
# Example output: Œ≤ = [50000, 80000, 5000]
# Meaning:
#   - Base price: $50,000
#   - Each bedroom adds: $80,000
#   - Each location point adds: $5,000

# Make a prediction for a new house: 3 bedrooms, location 8
new_house = np.array([1, 3, 8])
predicted_price = new_house @ beta
print(f"Predicted price: ${predicted_price:,.0f}")
```

---